{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest - Week 07 - Micro Credit\n",
    "\n",
    "Last week we introduced decision trees, which we used to predict credit approvals. This week we will extend the basic decision trees by two powerful ideas.  \n",
    "First we will have a look at __Bagging__ and then at __Random Forests__. Afterwards __Hyperparameter Tuning__ will be demonstrated and last but not least we will examine __Permutation based feature importance__ as an interpretability approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import random\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)  # Set seed for NumPy\n",
    "random.seed(42) # Set seed for random module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load the data\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/kbrennig/MODS_WS24_25/refs/heads/main/data/MicroCredit.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data\n",
    "\n",
    "First let's have a look at the raw data.\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.describe())\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Transform variables\n",
    "\n",
    "Similarly to last week, we drop the ID column.\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transformed = data.drop(columns=\"ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training and test sets\n",
    "\n",
    "Business as usual: We create a 80-20 training and test set.\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test = train_test_split(data_transformed, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Categorical Columns\n",
    "\n",
    "DecisionTreeClassifier, like many other machine learning models, requires __numerical input data__. If a dataset contains categorical variables or non-numeric data, they need to be __preprocessed before fitting the model__. Therefore categorical variables can be converted into numerical representations using techniques such as __one-hot encoding__. One-hot encoding creates binary columns for each category.\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# Dummy encoding / One-hot encoding\n",
    "\n",
    "categorical_columns = [\n",
    "    \"Decision\",\n",
    "    \"Tier\",\n",
    "    \"eom_25\",\n",
    "    \"OldEmi_d\",\n",
    "    \"BankSave_d\",\n",
    "    \"Build_Selfcon\",\n",
    "    \"Accommodation_Class\",\n",
    "    \"Loan_Type\",\n",
    "    \"Gender\",\n",
    "    \"Employment_Type\",\n",
    "    \"Doc_Proof_Inc\",\n",
    "    \"Marital_Status\",\n",
    "    \"Employer_Type\",\n",
    "    \"Education_Class\",\n",
    "    \"Mode_of_origin_class\"\n",
    "]\n",
    "\n",
    "# Creating the OneHotEncoder object\n",
    "encoder = OneHotEncoder(drop='first', handle_unknown='infrequent_if_exist', sparse_output=False)\n",
    "\n",
    "# Fit and transform the training data\n",
    "encoded_training = encoder.fit_transform(data_train.loc[:,categorical_columns])\n",
    "# Create a DataFrame with the encoded variables\n",
    "encoded_training_df = pd.DataFrame(encoded_training, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "# Join the encoded variables to the original DataFrame and remove the original columns\n",
    "data_train = data_train.reset_index(drop=True).join(encoded_training_df).drop(columns=categorical_columns)\n",
    "X_train = data_train.drop(columns=\"Decision_1\")\n",
    "y_train = data_train[\"Decision_1\"]\n",
    "\n",
    "\n",
    "# Transform the test data\n",
    "encoded_test = encoder.transform(data_test.loc[:,categorical_columns])\n",
    "encoded_test_df = pd.DataFrame(encoded_test, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "data_test = data_test.reset_index(drop=True).join(encoded_test_df).drop(columns=categorical_columns)\n",
    "X_test = data_test.drop(columns=\"Decision_1\")\n",
    "y_test = data_test[\"Decision_1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagged Trees by Hand\n",
    "\n",
    "Bagging (Bootstrap Aggregation) is a statistical method to __reduce variance__ and make predictions more stable. It describes the process of creating multiple __bootstrap samples__ from your training set and using them to train multiple models which are then aggregated to make predictions.  \n",
    "More specifically the steps for Training are:\n",
    "\n",
    "1. create m bootstrap samples by sampling from the training set.\n",
    "2. train m classifiers on the created samples.\n",
    "\n",
    "When we want to use the model to make predictions on new data, we proceed as follows:\n",
    "\n",
    "1. make m separate predictions on the new data.\n",
    "2. aggregate the m predictions by carrying out a __majory vote__. Majority voting outputs the class which occurs most often in the m predictions.\n",
    "\n",
    "![bagging.png](https://raw.githubusercontent.com/kbrennig/MODS_WS24_25/refs/heads/main/Week_7_exercise/images/bagging.png)\n",
    "<br>\n",
    "Source: Raschka (2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw 10 boostrap samples (sample with replacement)\n",
    "\n",
    "First we draw ten samples, using sampling with replacement. Each sample contains as many instances as the original training set (this doesn't mean that all the samples are the same, if unclear think again about sampling with replacement).\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of observations\n",
    "n_samples = data_train.shape[0]\n",
    "\n",
    "# Create 10 bootstrap samples\n",
    "bootsamp_01 = data_train.sample(replace=True, n=n_samples)\n",
    "bootsamp_01_X = bootsamp_01.drop(columns=\"Decision_1\")\n",
    "bootsamp_01_y = bootsamp_01[\"Decision_1\"]\n",
    "bootsamp_02 = data_train.sample(replace=True, n=n_samples)\n",
    "bootsamp_02_X = bootsamp_02.drop(columns=\"Decision_1\")\n",
    "bootsamp_02_y = bootsamp_02[\"Decision_1\"]\n",
    "bootsamp_03 = data_train.sample(replace=True, n=n_samples)\n",
    "bootsamp_03_X = bootsamp_03.drop(columns=\"Decision_1\")\n",
    "bootsamp_03_y = bootsamp_03[\"Decision_1\"]\n",
    "bootsamp_04 = data_train.sample(replace=True, n=n_samples)\n",
    "bootsamp_04_X = bootsamp_04.drop(columns=\"Decision_1\")\n",
    "bootsamp_04_y = bootsamp_04[\"Decision_1\"]\n",
    "bootsamp_05 = data_train.sample(replace=True, n=n_samples)\n",
    "bootsamp_05_X = bootsamp_05.drop(columns=\"Decision_1\")\n",
    "bootsamp_05_y = bootsamp_05[\"Decision_1\"]\n",
    "bootsamp_06 = data_train.sample(replace=True, n=n_samples)\n",
    "bootsamp_06_X = bootsamp_06.drop(columns=\"Decision_1\")\n",
    "bootsamp_06_y = bootsamp_06[\"Decision_1\"]\n",
    "bootsamp_07 = data_train.sample(replace=True, n=n_samples)\n",
    "bootsamp_07_X = bootsamp_07.drop(columns=\"Decision_1\")\n",
    "bootsamp_07_y = bootsamp_07[\"Decision_1\"]\n",
    "bootsamp_08 = data_train.sample(replace=True, n=n_samples)\n",
    "bootsamp_08_X = bootsamp_08.drop(columns=\"Decision_1\")\n",
    "bootsamp_08_y = bootsamp_08[\"Decision_1\"]\n",
    "bootsamp_09 = data_train.sample(replace=True, n=n_samples)\n",
    "bootsamp_09_X = bootsamp_09.drop(columns=\"Decision_1\")\n",
    "bootsamp_09_y = bootsamp_09[\"Decision_1\"]\n",
    "bootsamp_10 = data_train.sample(replace=True, n=n_samples)\n",
    "bootsamp_10_X = bootsamp_10.drop(columns=\"Decision_1\")\n",
    "bootsamp_10_y = bootsamp_10[\"Decision_1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grow 10 classification trees, each on a different bootstrap sample\n",
    "\n",
    "Then we train a decision trees on each sample, resulting in ten models.\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model_01 = DecisionTreeClassifier(random_state=42).fit(bootsamp_01_X, bootsamp_01_y)\n",
    "model_02 = DecisionTreeClassifier(random_state=42).fit(bootsamp_02_X, bootsamp_02_y)\n",
    "model_03 = DecisionTreeClassifier(random_state=42).fit(bootsamp_03_X, bootsamp_03_y)\n",
    "model_04 = DecisionTreeClassifier(random_state=42).fit(bootsamp_04_X, bootsamp_04_y)\n",
    "model_05 = DecisionTreeClassifier(random_state=42).fit(bootsamp_05_X, bootsamp_05_y)\n",
    "model_06 = DecisionTreeClassifier(random_state=42).fit(bootsamp_06_X, bootsamp_06_y)\n",
    "model_07 = DecisionTreeClassifier(random_state=42).fit(bootsamp_07_X, bootsamp_07_y)\n",
    "model_08 = DecisionTreeClassifier(random_state=42).fit(bootsamp_08_X, bootsamp_08_y)\n",
    "model_09 = DecisionTreeClassifier(random_state=42).fit(bootsamp_09_X, bootsamp_09_y)\n",
    "model_10 = DecisionTreeClassifier(random_state=42).fit(bootsamp_10_X, bootsamp_10_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot individual trees\n",
    "\n",
    "We can look at the resulting trees by plotting them.\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "models = [model_01, model_02, model_03, model_04, model_05, model_06, model_07, model_08, model_09, model_10]\n",
    "plot_size = (60, 30)  #define size of plot\n",
    "for model in models:\n",
    "    # Create a new figure\n",
    "    fig, ax = plt.subplots(figsize=plot_size)\n",
    "    # Plot the tree\n",
    "    plot_tree(model, ax=ax, feature_names=data_train.drop(columns=\"Decision_1\").columns, impurity=False, filled=True, rounded=True, proportion=True, class_names=True, fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make and aggregate individual predictions\n",
    "\n",
    "After we trained our models, we use them to make individual predictions on the training data.\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test_01 = model_01.predict_proba(X_test)\n",
    "predictions_test_01_binary = np.where(predictions_test_01[:, 1] > 0.5, 1, 0)\n",
    "predictions_test_02 = model_02.predict_proba(X_test)\n",
    "predictions_test_02_binary = np.where(predictions_test_02[:, 1] > 0.5, 1, 0)\n",
    "predictions_test_03 = model_03.predict_proba(X_test)\n",
    "predictions_test_03_binary = np.where(predictions_test_03[:, 1] > 0.5, 1, 0)\n",
    "predictions_test_04 = model_04.predict_proba(X_test)\n",
    "predictions_test_04_binary = np.where(predictions_test_04[:, 1] > 0.5, 1, 0)\n",
    "predictions_test_05 = model_05.predict_proba(X_test)\n",
    "predictions_test_05_binary = np.where(predictions_test_05[:, 1] > 0.5, 1, 0)\n",
    "predictions_test_06 = model_06.predict_proba(X_test)\n",
    "predictions_test_06_binary = np.where(predictions_test_06[:, 1] > 0.5, 1, 0)\n",
    "predictions_test_07 = model_07.predict_proba(X_test)\n",
    "predictions_test_07_binary = np.where(predictions_test_07[:, 1] > 0.5, 1, 0)\n",
    "predictions_test_08 = model_08.predict_proba(X_test)\n",
    "predictions_test_08_binary = np.where(predictions_test_08[:, 1] > 0.5, 1, 0)\n",
    "predictions_test_09 = model_09.predict_proba(X_test)\n",
    "predictions_test_09_binary = np.where(predictions_test_09[:, 1] > 0.5, 1, 0)\n",
    "predictions_test_10 = model_10.predict_proba(X_test)\n",
    "predictions_test_10_binary = np.where(predictions_test_10[:, 1] > 0.5, 1, 0)\n",
    "\n",
    "# Aggregate the predictions\n",
    "predictions_test_agg = (predictions_test_01[:, 1] + predictions_test_02[:, 1] + predictions_test_03[:, 1] + predictions_test_04[:, 1] + predictions_test_05[:, 1] + predictions_test_06[:, 1] + predictions_test_07[:, 1] + predictions_test_08[:, 1] + predictions_test_09[:, 1] + predictions_test_10[:, 1]) / 10\n",
    "predictions_test_agg_binary = np.where(predictions_test_agg > 0.5, 1, 0)\n",
    "\n",
    "data_test_w_preds = X_test.copy()\n",
    "data_test_w_preds['predictions_test_agg_binary'] = predictions_test_agg_binary\n",
    "data_test_w_preds['predictions_test_01_binary'] = predictions_test_01_binary\n",
    "data_test_w_preds['predictions_test_02_binary'] = predictions_test_02_binary\n",
    "data_test_w_preds['predictions_test_03_binary'] = predictions_test_03_binary\n",
    "data_test_w_preds['predictions_test_04_binary'] = predictions_test_04_binary\n",
    "data_test_w_preds['predictions_test_05_binary'] = predictions_test_05_binary\n",
    "data_test_w_preds['predictions_test_06_binary'] = predictions_test_06_binary\n",
    "data_test_w_preds['predictions_test_07_binary'] = predictions_test_07_binary\n",
    "data_test_w_preds['predictions_test_08_binary'] = predictions_test_08_binary\n",
    "data_test_w_preds['predictions_test_09_binary'] = predictions_test_09_binary\n",
    "data_test_w_preds['predictions_test_10_binary'] = predictions_test_10_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate mean accuracy of the 10 individual models on test set\n",
    "\n",
    "Lastly, we calculate two different accuracies. First we calculate the __mean accuracy over all models__.\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_mean = np.mean([accuracy_score(y_test, predictions_test_01_binary),\n",
    "                         accuracy_score(y_test, predictions_test_02_binary),\n",
    "                         accuracy_score(y_test, predictions_test_03_binary),\n",
    "                         accuracy_score(y_test, predictions_test_04_binary),\n",
    "                         accuracy_score(y_test, predictions_test_05_binary),\n",
    "                         accuracy_score(y_test, predictions_test_06_binary),\n",
    "                         accuracy_score(y_test, predictions_test_07_binary),\n",
    "                         accuracy_score(y_test, predictions_test_08_binary),\n",
    "                         accuracy_score(y_test, predictions_test_09_binary),\n",
    "                         accuracy_score(y_test, predictions_test_10_binary)])\n",
    "\n",
    "print(\"Mean accuracy over all models: \", accuracy_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate accuracy of the bagged trees on test set\n",
    "\n",
    "And secondly we calculate the __accuracy of the aggregated (averaged) predictions on the test set__.\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_agg = accuracy_score(y_test, predictions_test_agg_binary)\n",
    "\n",
    "print(\"Accuracy of the aggregated (averaged) predictions:\", accuracy_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clearly see the difference in accuracy and validate our assumptions that bagging leads to more stable and more accurate models.\n",
    "\n",
    "## Random Forest\n",
    "\n",
    "If we go back to the bagged trees, that we just created and look closely we will see that even though they were trained on different sets there are a lot of similarities between the trees. This is due to the fact, that there are a few very strong independent variables in our dataset that are chosen for most of the upper splits. Since our goal is to create as diverse models as possible this isn't a desired behavior.  \n",
    "\n",
    "Here __Random Forests__ come into play. Additionally to __Bagging__ they also __restrict the number of predictors__ that can be considered for each split in a tree. This is achieved by __randomly selecting a subset__ of all predictors __for each split__.\n",
    "\n",
    "### Grow random forest on training data\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Random Forests\n",
    "rf_01 = RandomForestClassifier(n_estimators=500, random_state=42).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show results\n",
    "\n",
    "We can see that there are different parameters that have to be specified in order to train a random forest. The __Number of trees__ (n_estimators) for example tells us how many trees have have been trained.   \n",
    "For a complete list of the `RandomForestClassifier` functions parameters, you can have a look at its [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Parameters:\", rf_01.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions and calculate evaluation metrics on test set\n",
    "\n",
    "Similarly to last week, we can make predictions on the test set and calculate different evaluation metrics.\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "predictions_testset_rf01 = rf_01.predict_proba(X_test)[:, 1]\n",
    "predictions_testset_rf01_binary = np.where(predictions_testset_rf01 > 0.5, 1, 0)\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy_rf = accuracy_score(y_test, predictions_testset_rf01_binary)\n",
    "print(\"Accuracy (Random Forests):\", accuracy_rf)\n",
    "\n",
    "# Create the confusion matrix\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, predictions_testset_rf01_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC and Auc\n",
    "\n",
    "Plot ROC curve and calculate AUC on test set.\n",
    "With __binary__ classification we get relatively straight lines. With the classification __probability__ we can map the distribution better. That is why we use the classification probability (e.g., predictions_testset_rf01) to calculate the AUC.\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay, roc_auc_score\n",
    "\n",
    "# Calculate and Print the AUC score\n",
    "auc_score = roc_auc_score(y_test, predictions_testset_rf01)\n",
    "print(\"AUC Score:\", auc_score)\n",
    "\n",
    "#plot ROC curve\n",
    "RocCurveDisplay.from_predictions(y_test, predictions_testset_rf01, plot_chance_level=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "If we look at the documentation of the `RandomForestClassifier` function we see that there are multiple __hyperparameters__ that can be specified and which will have an influence on our model's performance.   \n",
    "Instead of randomly choosing or adjusting these parameters we can make use of automated __Hyperparameter Tuning__.\n",
    "\n",
    "Tune __max_features__ and __max_depth__ hyperparameters, calculate the time it took for the hyperparameter tuning and use AUC as accuracy metric.\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Define parameter grid\n",
    "max_features = list(range(1, X_train.shape[1])) # Number of features to consider at every split\n",
    "max_depth = list(range(1, 20)) # Maximum number of levels in tree\n",
    "max_depth.append(None)\n",
    "\n",
    "param_grid = {\n",
    "    'max_features': max_features, \n",
    "    'max_depth': max_depth\n",
    "}\n",
    "\n",
    "# Tune hyperparameters\n",
    "rfc = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "rfc_random = RandomizedSearchCV(estimator=rfc, param_distributions=param_grid, n_iter=10, cv=5, scoring=\"roc_auc\", verbose=2, random_state=42, n_jobs=-1)\n",
    "rfc_random.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "# Print the best hyperparameters and the time taken\n",
    "print(\"Best parameters found:\", rfc_random.best_params_)\n",
    "print(\"Time taken for tuning:\", end_time - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show tuning results.\n",
    "\n",
    "After the tuning is done, we can look at the results that show us the obtained scores of different hyperparameter combinations.\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(rfc_random.cv_results_)\n",
    "print(results[['params', 'mean_test_score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the best hyperparameter combination.\n",
    "\n",
    "Additionally we can directly extract the best values and the best model.\n",
    "\n",
    "*Run the code below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rfc_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model with best parameters\n",
    "best_rfc = RandomForestClassifier(**rfc_random.best_params_, n_estimators=500, random_state=42)\n",
    "\n",
    "# Train model on training data\n",
    "best_rfc.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test data\n",
    "best_rfc_predictions = best_rfc.predict_proba(X_test) \n",
    "best_rfc_predictions_binary = np.where(best_rfc_predictions[:,1] > 0.5, 1, 0)\n",
    "\n",
    "# Calculate AUC\n",
    "auc_rfc_tuned = roc_auc_score(y_test, best_rfc_predictions[:,1]) # Here we also use the classification probability\n",
    "print(\"AUC Tuned RFC:\", auc_rfc_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model interpretability\n",
    "\n",
    "### Permutation based feature importance\n",
    "\n",
    "If done correctly, more complex ML models usually increase predictive performance, but on the other hand they drastically decrease transparency. This means that we can observe a model's in- and outputs but we are not able to understand how the model derived the outputs given the inputs. There are different __Interpretability approaches__ that try to reason about a model's behavior.   __Permutation based feature importance__ is one such approach. \n",
    "\n",
    "\"*Permutation feature importance measures the increase in the prediction error of the model after we permuted the feature's values, which breaks the relationship between the feature and the true outcome.*\" \n",
    "\n",
    "The permutation feature importance algorithm based on Fisher, Rudin, and Dominici (2018):\n",
    "\n",
    "Input: Trained model f, feature matrix X, target vector y, error measure L(y,f).\n",
    "\n",
    "1. Estimate the original model error e<sub>orig</sub> = L(y, f(X)) (e.g. mean squared error)  \n",
    "2. For each feature j = 1,...,p do:  \n",
    "    * Generate feature matrix X<sub>perm</sub> by permuting feature j in the data X. This breaks the association between feature j and true outcome y.  \n",
    "    * Estimate error e<sub>perm</sub> = L(Y,f(X<sub>perm</sub>)) based on the predictions of the permuted data.  \n",
    "    * Calculate permutation feature importance FI<sub>j</sub>= e<sub>perm</sub>/e<sub>orig</sub>.\n",
    "3. Sort features by descending FI.  \n",
    "\n",
    "[__Source: Molnar (2019)__](https://christophm.github.io/interpretable-ml-book/feature-importance.html)\n",
    "\n",
    "\n",
    "*Run the code below.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Calculate permutation-based feature importance\n",
    "result = permutation_importance(rf_01, X_train, y_train, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Get feature names from X_train\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Get the sorted indices of feature importance\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.barh(range(len(sorted_idx)), result.importances_mean[sorted_idx], xerr=result.importances_std[sorted_idx])\n",
    "plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])\n",
    "plt.xlabel('Permutation Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Permutation-based Feature Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "So to sum it up let us have a look what we did in this week's tutorial:\n",
    "\n",
    "1. we learned how __Bagging__ works and how it can improve model performance. \n",
    "2. after that, we had a look at __Random Forests__, which build on bagged trees and extend them by only using a __randomized subset of features__ at each split.\n",
    "3. the more complex the models get, the more __hyperparameters__ are to be defined. Instead of randomly choosing these hyperparameters it makes more sense to systematically try different combinations and choose the best performing one. This process is called __Hyperparameter Tuning__.\n",
    "4. Finally, we looked at __permutation based feature importance__, an attempt to make __blackbox ML models__ more interpretable to humans.\n",
    "\n",
    "\n",
    "\n",
    "*You can use the cell below to build and evaluate different models*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
