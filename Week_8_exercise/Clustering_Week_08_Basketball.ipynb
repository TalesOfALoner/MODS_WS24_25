{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "F7_dr5Lb-ahc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(42)  # Set seed for NumPy\n",
        "random.seed(42) # Set seed for random module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cojo84TC93P"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this weeks tutorial we will start with unsupervised learning. Until now we always had the goal to approximate the relationship of an output variable and multiple inputs, where the output as well as the inputs could be numerical or categorical.  \n",
        "\n",
        "In unsupervised learning there is no direct target variable, meaning we are not approximating a prediction function but are rather searching for interesting patterns within the given data.  \n",
        "One such unsupervised learning method is __Clustering__. In clustering we want to group (cluster) homogeneous instances while maximizing the heterogeneity of different clusters.\n",
        "\n",
        "For demonstration we will use a data set of [Basketball player statistics](https://www.kaggle.com/jacobbaruch/basketball-players-stats-per-season-49-leagues) which includes information about basketball players from all around the world."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GfzUB2_aD4pB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# Loading the data from a csv file\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/kbrennig/MODS_WS24_25/refs/heads/main/data/players_stats_by_season_full_details.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDy5l7kZC98n"
      },
      "source": [
        "## Explore Data\n",
        "First of all, let's have a look at the raw data. It seems like it is sorted by the league and the season, then for each player there is basic information like birthday and height. Additionally there are a bunch of interesting metrics for the game of basketball, which are shortly defined below:  \n",
        "\n",
        "- 'GP': Games Played\n",
        "- 'MIN': Minutes Played\n",
        "- 'FGM': Field Goals Made\n",
        "- 'FGA': Field Goals Attempts\n",
        "- '3PM': Three Points Made\n",
        "- '3PA': Three Points Attempts\n",
        "- 'FTM': Free Throws Made\n",
        "- 'FTA': Free Throws Attempts\n",
        "- 'TOV': Turnovers\n",
        "- 'PF': Personal Fouls\n",
        "- 'ORB': Offensive Rebounds\n",
        "- 'DRB': Defensive Rebounds\n",
        "- 'REB': Rebounds\n",
        "- 'AST': Assists\n",
        "- 'STL': Steals\n",
        "- 'BLK': Blocks\n",
        "- 'PTS': Points\n",
        "\n",
        "*Run the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0CuKt5mKEG4B",
        "outputId": "7793477e-0694-480c-c1d8-f154438db751",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 GP           MIN           FGM           FGA           3PM  \\\n",
            "count  36950.000000  36950.000000  36950.000000  36950.000000  36950.000000   \n",
            "mean      31.401083    781.178506    117.838268    254.699648     30.616969   \n",
            "std       16.112989    470.295971     91.730585    191.015086     30.658528   \n",
            "min        1.000000      0.700000      0.000000      0.000000      0.000000   \n",
            "25%       22.000000    476.000000     57.000000    131.000000      7.000000   \n",
            "50%       31.000000    726.000000     99.000000    217.000000     23.000000   \n",
            "75%       38.000000    983.200000    151.000000    323.000000     45.000000   \n",
            "max       83.000000   3239.300000    857.000000   1941.000000    402.000000   \n",
            "\n",
            "                3PA           FTM           FTA           TOV            PF  \\\n",
            "count  36950.000000  36950.000000  36950.000000  36950.000000  36950.000000   \n",
            "mean      87.107876     57.238484     77.404980     49.160162     72.323978   \n",
            "std       80.451297     53.446822     68.662186     34.383808     40.288340   \n",
            "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
            "25%       25.000000     23.000000     32.000000     26.000000     44.000000   \n",
            "50%       71.000000     44.000000     61.000000     43.000000     69.000000   \n",
            "75%      126.000000     75.000000    102.000000     65.000000     93.000000   \n",
            "max     1028.000000    756.000000    916.000000    464.000000    332.000000   \n",
            "\n",
            "       ...           DRB           REB           AST           STL  \\\n",
            "count  ...  36950.000000  36950.000000  36950.000000  36950.000000   \n",
            "mean   ...     94.218106    129.580650     65.333424     27.635372   \n",
            "std    ...     78.806687    109.112397     67.823130     20.710504   \n",
            "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
            "25%    ...     44.000000     59.000000     24.000000     13.000000   \n",
            "50%    ...     75.000000    102.000000     45.000000     23.000000   \n",
            "75%    ...    120.000000    166.000000     84.000000     37.000000   \n",
            "max    ...    848.000000   1247.000000    907.000000    191.000000   \n",
            "\n",
            "                BLK           PTS    birth_year     height_cm        weight  \\\n",
            "count  36950.000000  36950.000000  36774.000000  36912.000000  33919.000000   \n",
            "mean      10.383735    323.544060   1987.492549    197.165827    209.275332   \n",
            "std       16.272083    248.591981      4.913384      8.630686     25.669331   \n",
            "min        0.000000      0.000000   1968.000000    160.000000    130.000000   \n",
            "25%        1.000000    160.000000   1984.000000    191.000000    190.000000   \n",
            "50%        5.000000    275.000000   1988.000000    198.000000    207.000000   \n",
            "75%       13.000000    417.000000   1991.000000    203.000000    225.000000   \n",
            "max      269.000000   2818.000000   2002.000000    229.000000    375.000000   \n",
            "\n",
            "          weight_kg  \n",
            "count  33919.000000  \n",
            "mean      94.955836  \n",
            "std       11.643626  \n",
            "min       59.000000  \n",
            "25%       86.000000  \n",
            "50%       94.000000  \n",
            "75%      102.000000  \n",
            "max      170.000000  \n",
            "\n",
            "[8 rows x 21 columns]\n"
          ]
        }
      ],
      "source": [
        "print(data.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Koo2dvzMEGKt"
      },
      "source": [
        "## Transform Data\n",
        "\n",
        "### Filter the data and standardize columns\n",
        "\n",
        "When we look at the data, we quickly see, that it contains multiple records for one player. This most likely results from the fact that there are multiple season contained in the data. It could definitely be interesting to compare one player to previous versions of himself but for our purpose of clustering it is rather disadvantageous so we will filter the data for only one season. Additionally we will constrain the league to be the NBA.  \n",
        "So the resulting data contains information about players in the NBA in the 2018 - 2019 season.\n",
        "\n",
        "We also drop some columns since we will focus on a few key performance indicators.  \n",
        "\n",
        "And lastly we standardize the remaining columns, which is necessary for clustering when the variables have different units.\n",
        "\n",
        "*Run the code below.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ttuqK3WdFxht",
        "outputId": "1e55e5bd-b0fc-48cc-8b03-ebed4ac0cb0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 GP         FGM           3PM         FTM           TOV  \\\n",
            "count  4.350000e+02  435.000000  4.350000e+02  435.000000  4.350000e+02   \n",
            "mean  -6.533726e-17    0.000000  6.533726e-17    0.000000 -6.533726e-17   \n",
            "std    1.001151e+00    1.001151  1.001151e+00    1.001151  1.001151e+00   \n",
            "min   -1.616143e+00   -1.166936 -9.314751e-01   -0.921205 -1.087749e+00   \n",
            "25%   -1.178018e+00   -0.906121 -8.043562e-01   -0.716997 -8.456102e-01   \n",
            "50%    4.733766e-01   -0.161710 -2.958803e-01   -0.328029 -2.285462e-01   \n",
            "75%    8.777998e-01    0.560965  5.303929e-01    0.386699  4.822490e-01   \n",
            "max    1.113713e+00    3.413632  5.074896e+00    6.410836  4.957916e+00   \n",
            "\n",
            "                 PF           REB           AST           STL           BLK  \\\n",
            "count  4.350000e+02  4.350000e+02  4.350000e+02  4.350000e+02  4.350000e+02   \n",
            "mean  -1.960118e-16 -6.533726e-17  1.306745e-16  6.533726e-17  3.266863e-17   \n",
            "std    1.001151e+00  1.001151e+00  1.001151e+00  1.001151e+00  1.001151e+00   \n",
            "min   -1.452677e+00 -1.120729e+00 -9.442424e-01 -1.162260e+00 -8.330224e-01   \n",
            "25%   -1.020889e+00 -8.306350e-01 -7.690797e-01 -8.937614e-01 -6.706435e-01   \n",
            "50%    6.554464e-02 -1.827590e-01 -3.106751e-01 -1.777647e-01 -3.458857e-01   \n",
            "75%    8.316196e-01  4.433599e-01  4.048834e-01  5.978984e-01  2.711541e-01   \n",
            "max    2.614485e+00  4.835862e+00  4.899485e+00  3.909383e+00  5.629657e+00   \n",
            "\n",
            "                PTS  \n",
            "count  4.350000e+02  \n",
            "mean   6.533726e-17  \n",
            "std    1.001151e+00  \n",
            "min   -1.149800e+00  \n",
            "25%   -8.864532e-01  \n",
            "50%   -1.970183e-01  \n",
            "75%    5.634313e-01  \n",
            "max    4.409077e+00  \n"
          ]
        }
      ],
      "source": [
        "# Filter the data for NBA 2018-2019 season\n",
        "dataset_for_clustering = data[(data['League'] == 'NBA') & (data['Season'] == '2018 - 2019')]\n",
        "\n",
        "# Drop irrelevant columns\n",
        "dataset_for_clustering = dataset_for_clustering.drop(columns=['high_school', 'nationality', 'weight', 'height', 'height_cm',\n",
        "                                                              'weight_kg', 'birth_date', 'birth_month', 'birth_year',\n",
        "                                                              'League', 'Season', 'Stage', 'FGA', '3PA', 'FTA', 'ORB',\n",
        "                                                              'DRB', 'MIN'])\n",
        "\n",
        "# Separate players and teams\n",
        "players_and_teams = dataset_for_clustering[['Player', 'Team']]\n",
        "dataset_for_clustering = dataset_for_clustering.drop(columns=['Player', 'Team'])\n",
        "\n",
        "# Standardize the data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "dataset_for_clustering_scaled = pd.DataFrame(scaler.fit_transform(dataset_for_clustering), columns=dataset_for_clustering.columns)\n",
        "\n",
        "# Show summary of transformed data\n",
        "print(dataset_for_clustering_scaled.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9hMJhz0C9_r"
      },
      "source": [
        "## K-means Clustering\n",
        "\n",
        "In the lecture two clustering approaches were explained. We will start with __K-means clustering__.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1TSCDN3GhUM"
      },
      "source": [
        "### Perform clustering\n",
        "\n",
        "*Run the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uMYnDtzbGO7f"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Perform K-means clustering with 4 clusters\n",
        "kmeans = KMeans(n_clusters=4, n_init='auto', max_iter=300, random_state=42)\n",
        "kmeans_cluster_model = kmeans.fit(dataset_for_clustering_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOGbsCG0C-DD"
      },
      "source": [
        "### Extract results\n",
        "\n",
        "*Run the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dqSe455gGRZQ"
      },
      "outputs": [],
      "source": [
        "km_clusters = kmeans.labels_\n",
        "# Extract centroids\n",
        "centroids = kmeans.cluster_centers_.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJwkST1XC-GB"
      },
      "source": [
        "### Calculate silhoutte scores\n",
        "\n",
        "Evaluating the performance of an unsupervised learning method is different compared to a supervised method.  \n",
        "Since we are trying to learn a function in supervised learning and we are given the actual outputs of this (unknown) function we can simply compare the outputs of our learned function to the actual outputs and thereby relatively intuitively assess our learned models performance.\n",
        "\n",
        "In clustering the goal is to cluster similar instances together and maximize the distance between clusters. One way to evaluate this kind of procedure is to look at the __distances__ and __compare__ for example the __distances__ an instance has to other instances in its __own cluster__ and to instances of __other clusters__.\n",
        "\n",
        "The __silhouette score__ does exactly that.\n",
        "\n",
        "*Run the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PRdtNjcMGqO_",
        "outputId": "e1d71d8d-f2c5-4481-d198-8ef14a4b042c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Silhouette Score: 0.4307221457593341\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Calculate silhouette score\n",
        "silhouette_score_avg_kmeans = silhouette_score(dataset_for_clustering_scaled, km_clusters)\n",
        "print(f\"Mean Silhouette Score: {silhouette_score_avg_kmeans}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBcg5JUmGxwg"
      },
      "source": [
        "### Find the optimal number of clusters\n",
        "\n",
        "The number of clusters is a __hyperparameter__ which has to be explored. We can for example simply repeat the clustering for different numbers of clusters and compare their respective silhouette scores to determine, which number yields the best clustering.\n",
        "\n",
        "*Run the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wt2ELVyWC8-Q",
        "outputId": "6b0d77a1-c345-4f51-ea34-b4891dfeee36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of clusters: 2 - Silhouette score: 0.43732362828201393\n",
            "Number of clusters: 3 - Silhouette score: 0.4348158150986768\n",
            "Number of clusters: 4 - Silhouette score: 0.4307221457593341\n",
            "Number of clusters: 5 - Silhouette score: 0.41019524458784673\n",
            "Number of clusters: 6 - Silhouette score: 0.3750553915467234\n",
            "Number of clusters: 7 - Silhouette score: 0.3750773518920066\n",
            "Number of clusters: 8 - Silhouette score: 0.36910356711115894\n",
            "Number of clusters: 9 - Silhouette score: 0.37072005478414466\n",
            "Number of clusters: 10 - Silhouette score: 0.34381970236548176\n",
            "Number of clusters: 11 - Silhouette score: 0.33199603098893815\n",
            "Number of clusters: 12 - Silhouette score: 0.3338751232969291\n",
            "Number of clusters: 13 - Silhouette score: 0.3250560760355024\n",
            "Number of clusters: 14 - Silhouette score: 0.32558430351812817\n",
            "Number of clusters: 15 - Silhouette score: 0.32132960231424285\n"
          ]
        }
      ],
      "source": [
        "# Iterate over different numbers of clusters and calculate silhouette score\n",
        "for i in range(2, 16):\n",
        "    kmeans = KMeans(n_clusters=i, n_init='auto', max_iter=300, random_state=42)\n",
        "    kmeans_cluster_model = kmeans.fit(dataset_for_clustering_scaled)\n",
        "    km_clusters = kmeans.labels_\n",
        "\n",
        "    silhouette_score_avg_kmeans = silhouette_score(dataset_for_clustering_scaled, km_clusters)\n",
        "    print(f\"Number of clusters: {i} - Silhouette score: {silhouette_score_avg_kmeans}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8TUTTSMG5w4"
      },
      "source": [
        "### Visualize Clustering Results\n",
        "Let's visualize the clustering results using a bar chart of the cluster centroids.\n",
        "\n",
        "The code below depicts the resulting cluster centroids for two clusters (optimal number of clusters determined in previous step).\n",
        "\n",
        "+ *How would you interpret the clustering results?*\n",
        "\n",
        "*Run the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGeXjeZkG_3T"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "# recalculate the model for two clusters\n",
        "kmeans = KMeans(n_clusters=2, n_init='auto', max_iter=300, random_state=42)\n",
        "kmeans_cluster_model = kmeans.fit(dataset_for_clustering_scaled)\n",
        "km_clusters = kmeans.labels_\n",
        "centroids = kmeans.cluster_centers_.T\n",
        "\n",
        "# Plot the centroids for the clusters\n",
        "centroids_df = pd.DataFrame(centroids.T, columns=dataset_for_clustering_scaled.columns)\n",
        "clusters = [1, 2]\n",
        "\n",
        "fig = go.Figure()\n",
        "for column in centroids_df.columns:\n",
        "    fig.add_trace(go.Bar(x=clusters, y=centroids_df[column], name=column))\n",
        "\n",
        "fig.update_layout(yaxis_title='Count', barmode='group', title='K-means Clustering Results')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFsndZVgHApJ"
      },
      "source": [
        "## Hierarchical Clustering\n",
        "The second method presented in the lecture is **Hierarchical clustering**.\n",
        "In **K-means clustering** we are randomly choosing centroids and try to cluster our data around these clusters.\n",
        "\n",
        "Hierarchical clustering works different. Here we start by computing the **pairwise distances** (and later **intercluster distances**) for all instances and then iteratively combine the instances (and later clusters) which are the closest to each other into one cluster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWYnVvvQgDck"
      },
      "source": [
        "### Perform clustering\n",
        "\n",
        "*Run the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2yp9OyvHQZa"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import pdist\n",
        "from scipy.cluster.hierarchy import linkage\n",
        "\n",
        "# Calculate pairwise distances using Euclidean distance\n",
        "distances = pdist(dataset_for_clustering_scaled, metric='euclidean')\n",
        "\n",
        "# Perform hierarchical clustering using complete linkage\n",
        "hcluster_model = linkage(distances, method='complete')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42Qbz7-FHQ0w"
      },
      "source": [
        "### Plot Dendrogram\n",
        "Apart from the clustering itself, hierarchical clustering also produces a **dendrogram** as a side product. The dendrogram visualizes the order in which clusters are merged. Though it may be cluttered, the top part can show interesting insights.\n",
        "\n",
        "*Run the code below.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVEtr3ZiHTwx"
      },
      "outputs": [],
      "source": [
        "from scipy.cluster.hierarchy import dendrogram\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot dendrogram\n",
        "plt.figure(figsize=(20, 7))\n",
        "dendrogram(\n",
        "    hcluster_model,\n",
        "    labels=players_and_teams['Player'].values,\n",
        "    leaf_rotation=90,\n",
        "    leaf_font_size=5\n",
        ")\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX4CKWcVHUNX"
      },
      "source": [
        "### Cut Dendrogram\n",
        "Once the hierarchical clustering is finished, we can extract clusterings for different numbers of clusters by simply cutting the dendrogram at the right position.\n",
        "\n",
        "*Run the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPqzoZfnHV7s"
      },
      "outputs": [],
      "source": [
        "from scipy.cluster.hierarchy import cut_tree\n",
        "\n",
        "# Cut dendrogram to form clusters\n",
        "h_clusters = cut_tree(hcluster_model, n_clusters=2).flatten()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O47XcvpNgDcl"
      },
      "source": [
        "### Plot Dendrogram for two clusters\n",
        "\n",
        "To visualize the clustering, we add a horizontal line at the threshold where the dendrogram is cut. This line highlights the level at which the clusters are formed, with branches below it automatically colored to indicate the distinct clusters.\n",
        "\n",
        "*Run the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBUAariwgDcl"
      },
      "outputs": [],
      "source": [
        "# Compute the flat clusters to confirm there are two clusters\n",
        "num_clusters = 2\n",
        "\n",
        "# Find the appropriate distance threshold for two clusters\n",
        "# The threshold is typically the height at which the tree splits into the desired number of clusters\n",
        "color_threshold = hcluster_model[-(num_clusters - 1), 2]  # The height of the last merge that creates 2 clusters\n",
        "\n",
        "# Plot the dendrogram with the color threshold\n",
        "plt.figure(figsize=(20, 7))\n",
        "dendrogram(\n",
        "    hcluster_model,\n",
        "    labels=players_and_teams['Player'].values,\n",
        "    leaf_rotation=90,\n",
        "    leaf_font_size=5,\n",
        "    color_threshold=color_threshold\n",
        ")\n",
        "plt.title('Hierarchical Clustering Dendrogram (Colored by 2 Clusters)')\n",
        "plt.axhline(y=color_threshold, c='black', linestyle='--', lw=1)  # Add a line for the cut-off\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6mEpcmdHX6f"
      },
      "source": [
        "### Calculate Silhouette Score\n",
        "We'll evaluate the hierarchical clustering using the silhouette score.\n",
        "\n",
        "*Run the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjh5IXCmHZ9A"
      },
      "outputs": [],
      "source": [
        "# Calculate silhouette score for hierarchical clustering\n",
        "silhouette_score_avg_hc = silhouette_score(dataset_for_clustering_scaled, h_clusters)\n",
        "print(f\"Mean Silhouette Score for Hierarchical Clustering: {silhouette_score_avg_hc}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTbuXU59HcHD"
      },
      "source": [
        "### Find the Optimal Number of Clusters for Hierarchical Clustering\n",
        "Similar to K-means, we can experiment with different cluster numbers to find the optimal configuration.\n",
        "\n",
        "*Run the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6JDpRDjHeNj"
      },
      "outputs": [],
      "source": [
        "# Iterate over different numbers of clusters and calculate silhouette score for hierarchical clustering\n",
        "for i in range(2, 16):\n",
        "    h_clusters = cut_tree(hcluster_model, n_clusters=i).flatten()\n",
        "    silhouette_score_avg_hc = silhouette_score(dataset_for_clustering_scaled, h_clusters)\n",
        "    print(f\"Number of clusters: {i} - Mean Silhouette score: {silhouette_score_avg_hc}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1eJvJ2UHfzN"
      },
      "source": [
        "## Summary\n",
        "In this tutorial, we covered:\n",
        "1. Performing K-means clustering and evaluating it using silhouette scores.\n",
        "2. Visualizing clusters by plotting centroids.\n",
        "3. Performing hierarchical clustering and visualizing it with a dendrogram.\n",
        "\n",
        "\n",
        "*You can use the cell below to build and evaluate different clusterings*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "GgaRMkefyKTO",
        "outputId": "dc562061-a8fa-4514-9c85-472a2569ca03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Silhouette Score: 0.37072005478414466\n",
            "Number of clusters: 4 - Silhouette score: 0.4307221457593341\n",
            "Number of clusters: 8 - Silhouette score: 0.36910356711115894\n",
            "Number of clusters: 9 - Silhouette score: 0.37072005478414466\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Calculate silhouette score\n",
        "silhouette_score_avg_kmeans = silhouette_score(dataset_for_clustering_scaled, km_clusters)\n",
        "print(f\"Mean Silhouette Score: {silhouette_score_avg_kmeans}\")\n",
        "\n",
        "list1 = [4,8,9]\n",
        "# Iterate over different numbers of clusters and calculate silhouette score\n",
        "for i in list1:\n",
        "    kmeans = KMeans(n_clusters=i, n_init='auto', max_iter=300, random_state=42)\n",
        "    kmeans_cluster_model = kmeans.fit(dataset_for_clustering_scaled)\n",
        "    km_clusters = kmeans.labels_\n",
        "\n",
        "    silhouette_score_avg_kmeans = silhouette_score(dataset_for_clustering_scaled, km_clusters)\n",
        "    print(f\"Number of clusters: {i} - Silhouette score: {silhouette_score_avg_kmeans}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "MODS",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}