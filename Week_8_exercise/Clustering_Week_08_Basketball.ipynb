{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"F7_dr5Lb-ahc"},"outputs":[],"source":["import numpy as np\n","import random\n","# Set seed for reproducibility\n","np.random.seed(42)  # Set seed for NumPy\n","random.seed(42) # Set seed for random module"]},{"cell_type":"markdown","metadata":{"id":"3cojo84TC93P"},"source":["## Introduction\n","\n","In this weeks tutorial we will start with unsupervised learning. Until now we always had the goal to approximate the relationship of an output variable and multiple inputs, where the output as well as the inputs could be numerical or categorical.  \n","\n","In unsupervised learning there is no direct target variable, meaning we are not approximating a prediction function but are rather searching for interesting patterns within the given data.  \n","One such unsupervised learning method is __Clustering__. In clustering we want to group (cluster) homogeneous instances while maximizing the heterogeneity of different clusters.\n","\n","For demonstration we will use a data set of [Basketball player statistics](https://www.kaggle.com/jacobbaruch/basketball-players-stats-per-season-49-leagues) which includes information about basketball players from all around the world."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GfzUB2_aD4pB"},"outputs":[],"source":["import pandas as pd\n","# Loading the data from a csv file\n","data = pd.read_csv(\"https://raw.githubusercontent.com/kbrennig/MODS_WS24_25/refs/heads/main/data/players_stats_by_season_full_details.csv\")"]},{"cell_type":"markdown","metadata":{"id":"dDy5l7kZC98n"},"source":["## Explore Data\n","First of all, let's have a look at the raw data. It seems like it is sorted by the league and the season, then for each player there is basic information like birthday and height. Additionally there are a bunch of interesting metrics for the game of basketball, which are shortly defined below:  \n","\n","- 'GP': Games Played\n","- 'MIN': Minutes Played\n","- 'FGM': Field Goals Made\n","- 'FGA': Field Goals Attempts\n","- '3PM': Three Points Made\n","- '3PA': Three Points Attempts\n","- 'FTM': Free Throws Made\n","- 'FTA': Free Throws Attempts\n","- 'TOV': Turnovers\n","- 'PF': Personal Fouls\n","- 'ORB': Offensive Rebounds\n","- 'DRB': Defensive Rebounds\n","- 'REB': Rebounds\n","- 'AST': Assists\n","- 'STL': Steals\n","- 'BLK': Blocks\n","- 'PTS': Points\n","\n","*Run the code below.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0CuKt5mKEG4B"},"outputs":[],"source":["print(data.describe())"]},{"cell_type":"markdown","metadata":{"id":"Koo2dvzMEGKt"},"source":["## Transform Data\n","\n","### Filter the data and standardize columns\n","\n","When we look at the data, we quickly see, that it contains multiple records for one player. This most likely results from the fact that there are multiple season contained in the data. It could definitely be interesting to compare one player to previous versions of himself but for our purpose of clustering it is rather disadvantageous so we will filter the data for only one season. Additionally we will constrain the league to be the NBA.  \n","So the resulting data contains information about players in the NBA in the 2018 - 2019 season.\n","\n","We also drop some columns since we will focus on a few key performance indicators.  \n","\n","And lastly we standardize the remaining columns, which is necessary for clustering when the variables have different units.\n","\n","*Run the code below.*\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ttuqK3WdFxht"},"outputs":[],"source":["# Filter the data for NBA 2018-2019 season\n","dataset_for_clustering = data[(data['League'] == 'NBA') & (data['Season'] == '2018 - 2019')]\n","\n","# Drop irrelevant columns\n","dataset_for_clustering = dataset_for_clustering.drop(columns=['high_school', 'nationality', 'weight', 'height', 'height_cm',\n","                                                              'weight_kg', 'birth_date', 'birth_month', 'birth_year',\n","                                                              'League', 'Season', 'Stage', 'FGA', '3PA', 'FTA', 'ORB',\n","                                                              'DRB', 'MIN'])\n","\n","# Separate players and teams\n","players_and_teams = dataset_for_clustering[['Player', 'Team']]\n","dataset_for_clustering = dataset_for_clustering.drop(columns=['Player', 'Team'])\n","\n","# Standardize the data\n","from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","dataset_for_clustering_scaled = pd.DataFrame(scaler.fit_transform(dataset_for_clustering), columns=dataset_for_clustering.columns)\n","\n","# Show summary of transformed data\n","print(dataset_for_clustering_scaled.describe())"]},{"cell_type":"markdown","metadata":{"id":"H9hMJhz0C9_r"},"source":["## K-means Clustering\n","\n","In the lecture two clustering approaches were explained. We will start with __K-means clustering__.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"A1TSCDN3GhUM"},"source":["### Perform clustering\n","\n","*Run the code below.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uMYnDtzbGO7f"},"outputs":[],"source":["from sklearn.cluster import KMeans\n","\n","# Perform K-means clustering with 4 clusters\n","kmeans = KMeans(n_clusters=4, n_init='auto', max_iter=300, random_state=42)\n","kmeans_cluster_model = kmeans.fit(dataset_for_clustering_scaled)"]},{"cell_type":"markdown","metadata":{"id":"AOGbsCG0C-DD"},"source":["### Extract results\n","\n","*Run the code below.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dqSe455gGRZQ"},"outputs":[],"source":["km_clusters = kmeans.labels_\n","# Extract centroids\n","centroids = kmeans.cluster_centers_.T"]},{"cell_type":"markdown","metadata":{"id":"zJwkST1XC-GB"},"source":["### Calculate silhoutte scores\n","\n","Evaluating the performance of an unsupervised learning method is different compared to a supervised method.  \n","Since we are trying to learn a function in supervised learning and we are given the actual outputs of this (unknown) function we can simply compare the outputs of our learned function to the actual outputs and thereby relatively intuitively assess our learned models performance.\n","\n","In clustering the goal is to cluster similar instances together and maximize the distance between clusters. One way to evaluate this kind of procedure is to look at the __distances__ and __compare__ for example the __distances__ an instance has to other instances in its __own cluster__ and to instances of __other clusters__.\n","\n","The __silhouette score__ does exactly that.\n","\n","*Run the code below.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PRdtNjcMGqO_"},"outputs":[],"source":["from sklearn.metrics import silhouette_score\n","\n","# Calculate silhouette score\n","silhouette_score_avg_kmeans = silhouette_score(dataset_for_clustering_scaled, km_clusters)\n","print(f\"Mean Silhouette Score: {silhouette_score_avg_kmeans}\")"]},{"cell_type":"markdown","metadata":{"id":"aBcg5JUmGxwg"},"source":["### Find the optimal number of clusters\n","\n","The number of clusters is a __hyperparameter__ which has to be explored. We can for example simply repeat the clustering for different numbers of clusters and compare their respective silhouette scores to determine, which number yields the best clustering.\n","\n","*Run the code below.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wt2ELVyWC8-Q"},"outputs":[],"source":["# Iterate over different numbers of clusters and calculate silhouette score\n","for i in range(2, 16):\n","    kmeans = KMeans(n_clusters=i, n_init='auto', max_iter=300, random_state=42)\n","    kmeans_cluster_model = kmeans.fit(dataset_for_clustering_scaled)\n","    km_clusters = kmeans.labels_\n","\n","    silhouette_score_avg_kmeans = silhouette_score(dataset_for_clustering_scaled, km_clusters)\n","    print(f\"Number of clusters: {i} - Silhouette score: {silhouette_score_avg_kmeans}\")"]},{"cell_type":"markdown","metadata":{"id":"R8TUTTSMG5w4"},"source":["### Visualize Clustering Results\n","Let's visualize the clustering results using a bar chart of the cluster centroids.\n","\n","The code below depicts the resulting cluster centroids for two clusters (optimal number of clusters determined in previous step).\n","\n","+ *How would you interpret the clustering results?*\n","\n","*Run the code below.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WGeXjeZkG_3T"},"outputs":[],"source":["import plotly.graph_objects as go\n","\n","# recalculate the model for two clusters\n","kmeans = KMeans(n_clusters=2, n_init='auto', max_iter=300, random_state=42)\n","kmeans_cluster_model = kmeans.fit(dataset_for_clustering_scaled)\n","km_clusters = kmeans.labels_\n","centroids = kmeans.cluster_centers_.T\n","\n","# Plot the centroids for the clusters\n","centroids_df = pd.DataFrame(centroids.T, columns=dataset_for_clustering_scaled.columns)\n","clusters = [1, 2]\n","\n","fig = go.Figure()\n","for column in centroids_df.columns:\n","    fig.add_trace(go.Bar(x=clusters, y=centroids_df[column], name=column))\n","\n","fig.update_layout(yaxis_title='Count', barmode='group', title='K-means Clustering Results')\n","fig.show()"]},{"cell_type":"markdown","metadata":{"id":"lFsndZVgHApJ"},"source":["## Hierarchical Clustering\n","The second method presented in the lecture is **Hierarchical clustering**.\n","In **K-means clustering** we are randomly choosing centroids and try to cluster our data around these clusters.\n","\n","Hierarchical clustering works different. Here we start by computing the **pairwise distances** (and later **intercluster distances**) for all instances and then iteratively combine the instances (and later clusters) which are the closest to each other into one cluster."]},{"cell_type":"markdown","metadata":{},"source":["### Perform clustering\n","\n","*Run the code below.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V2yp9OyvHQZa"},"outputs":[],"source":["from scipy.spatial.distance import pdist\n","from scipy.cluster.hierarchy import linkage\n","\n","# Calculate pairwise distances using Euclidean distance\n","distances = pdist(dataset_for_clustering_scaled, metric='euclidean')\n","\n","# Perform hierarchical clustering using complete linkage\n","hcluster_model = linkage(distances, method='complete')"]},{"cell_type":"markdown","metadata":{"id":"42Qbz7-FHQ0w"},"source":["### Plot Dendrogram\n","Apart from the clustering itself, hierarchical clustering also produces a **dendrogram** as a side product. The dendrogram visualizes the order in which clusters are merged. Though it may be cluttered, the top part can show interesting insights.\n","\n","*Run the code below.*\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JVEtr3ZiHTwx"},"outputs":[],"source":["from scipy.cluster.hierarchy import dendrogram\n","import matplotlib.pyplot as plt\n","\n","# Plot dendrogram\n","plt.figure(figsize=(20, 7))\n","dendrogram(\n","    hcluster_model, \n","    labels=players_and_teams['Player'].values, \n","    leaf_rotation=90, \n","    leaf_font_size=5\n",")\n","plt.title('Hierarchical Clustering Dendrogram')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"zX4CKWcVHUNX"},"source":["### Cut Dendrogram\n","Once the hierarchical clustering is finished, we can extract clusterings for different numbers of clusters by simply cutting the dendrogram at the right position.\n","\n","*Run the code below.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SPqzoZfnHV7s"},"outputs":[],"source":["from scipy.cluster.hierarchy import cut_tree\n","\n","# Cut dendrogram to form clusters\n","h_clusters = cut_tree(hcluster_model, n_clusters=2).flatten()\n"]},{"cell_type":"markdown","metadata":{},"source":["### Plot Dendrogram for two clusters\n","\n","To visualize the clustering, we add a horizontal line at the threshold where the dendrogram is cut. This line highlights the level at which the clusters are formed, with branches below it automatically colored to indicate the distinct clusters.\n","\n","*Run the code below.*"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Compute the flat clusters to confirm there are two clusters\n","num_clusters = 2\n","\n","# Find the appropriate distance threshold for two clusters\n","# The threshold is typically the height at which the tree splits into the desired number of clusters\n","color_threshold = hcluster_model[-(num_clusters - 1), 2]  # The height of the last merge that creates 2 clusters\n","\n","# Plot the dendrogram with the color threshold\n","plt.figure(figsize=(20, 7))\n","dendrogram(\n","    hcluster_model,\n","    labels=players_and_teams['Player'].values,\n","    leaf_rotation=90,\n","    leaf_font_size=5,\n","    color_threshold=color_threshold\n",")\n","plt.title('Hierarchical Clustering Dendrogram (Colored by 2 Clusters)')\n","plt.axhline(y=color_threshold, c='black', linestyle='--', lw=1)  # Add a line for the cut-off\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"R6mEpcmdHX6f"},"source":["### Calculate Silhouette Score\n","We'll evaluate the hierarchical clustering using the silhouette score.\n","\n","*Run the code below.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jjh5IXCmHZ9A"},"outputs":[],"source":["# Calculate silhouette score for hierarchical clustering\n","silhouette_score_avg_hc = silhouette_score(dataset_for_clustering_scaled, h_clusters)\n","print(f\"Mean Silhouette Score for Hierarchical Clustering: {silhouette_score_avg_hc}\")\n"]},{"cell_type":"markdown","metadata":{"id":"JTbuXU59HcHD"},"source":["### Find the Optimal Number of Clusters for Hierarchical Clustering\n","Similar to K-means, we can experiment with different cluster numbers to find the optimal configuration.\n","\n","*Run the code below.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_6JDpRDjHeNj"},"outputs":[],"source":["# Iterate over different numbers of clusters and calculate silhouette score for hierarchical clustering\n","for i in range(2, 16):\n","    h_clusters = cut_tree(hcluster_model, n_clusters=i).flatten()\n","    silhouette_score_avg_hc = silhouette_score(dataset_for_clustering_scaled, h_clusters)\n","    print(f\"Number of clusters: {i} - Mean Silhouette score: {silhouette_score_avg_hc}\")\n"]},{"cell_type":"markdown","metadata":{"id":"U1eJvJ2UHfzN"},"source":["## Summary\n","In this tutorial, we covered:\n","1. Performing K-means clustering and evaluating it using silhouette scores.\n","2. Visualizing clusters by plotting centroids.\n","3. Performing hierarchical clustering and visualizing it with a dendrogram.\n","\n","\n","*You can use the cell below to build and evaluate different clusterings*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GgaRMkefyKTO"},"outputs":[],"source":["# Enter your Code here!"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"MODS","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}
