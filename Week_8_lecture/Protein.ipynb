{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, cut_tree\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from a tab-separated text file\n",
    "data = pd.read_csv(\"../data/protein.txt\", sep=\"\\t\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove categorical columns and normalize (center and scale) the numerical variables\n",
    "data_prep = data.drop(columns=['Country'])\n",
    "scaler = StandardScaler()\n",
    "data_prep_scaled = scaler.fit_transform(data_prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perform k-means clustering with k=5\n",
    "kmeans_cluster_model = KMeans(n_clusters=5, n_init=20, random_state=42)\n",
    "kmeans_clusters = kmeans_cluster_model.fit_predict(data_prep_scaled)\n",
    "\n",
    "# Add cluster assignments to the original data\n",
    "data_prep_w_kmeans_clusters = data.copy()\n",
    "data_prep_w_kmeans_clusters['kmeans_clusters'] = kmeans_clusters\n",
    "\n",
    "# Get cluster centroids\n",
    "centroids = kmeans_cluster_model.cluster_centers_.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hierarchical clustering\n",
    "distances = pdist(data_prep_scaled, metric='euclidean')\n",
    "h_cluster_model = linkage(distances, method='complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 7))\n",
    "dendrogram(h_cluster_model, color_threshold=0, labels=data.iloc[:, 0].values) \n",
    "plt.title('Dendrogram for Hierarchical Clustering')\n",
    "plt.xlabel('Sample index')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut the dendrogram at k=5 and extract cluster assignments\n",
    "k = 5\n",
    "h_clusters = cut_tree(h_cluster_model, n_clusters=k).flatten()  # Flatten to get a 1D array\n",
    "data_prep_w_h_clusters = data.copy()\n",
    "data_prep_w_h_clusters['h_clusters'] = h_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Clusters via their Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 7\n",
    "h_clusters = cut_tree(h_cluster_model, n_clusters=k).flatten()  # Flatten to get a 1D array\n",
    "\n",
    "# Create a DataFrame to hold the original data and cluster assignments\n",
    "data_prep_w_h_clusters = data.copy()\n",
    "data_prep_w_h_clusters['h_clusters'] = h_clusters\n",
    "\n",
    "# Sort the DataFrame by the 'h_clusters' column\n",
    "data_prep_w_h_clusters_sorted = data_prep_w_h_clusters.sort_values(by='h_clusters')\n",
    "data_prep_w_h_clusters_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 1: Calculate centroids\n",
    "centroids = []\n",
    "for cluster in range(k):\n",
    "    cluster_points = data_prep_scaled[h_clusters == cluster]\n",
    "    centroid = cluster_points.mean(axis=0)\n",
    "    centroids.append(centroid)\n",
    "\n",
    "centroids = np.array(centroids)\n",
    "\n",
    "# Step 2: Create a DataFrame for centroids\n",
    "centroids_df = pd.DataFrame(centroids, columns=data.columns[1:])  # Adjust if needed\n",
    "centroids_df['Cluster'] = range(k)\n",
    "\n",
    "# Step 3: Create bar plots for each cluster\n",
    "for cluster in range(k):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(centroids_df.columns[:-1], centroids_df.iloc[cluster, :-1], color='skyblue')\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Distance from Centroid')\n",
    "    plt.title(f'Distances from Centroid of Cluster {cluster}')\n",
    "    plt.xticks(rotation=45)  # Rotate feature names for better visibility\n",
    "    plt.grid(axis='y')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Step 4: Display the DataFrame for the corresponding cluster\n",
    "    cluster_data = data_prep_w_h_clusters_sorted[data_prep_w_h_clusters_sorted['h_clusters'] == cluster]\n",
    "    print(f'Data for Cluster {cluster}:')\n",
    "    print(cluster_data)\n",
    "    print('\\n' + '-'*50 + '\\n')  # Separator for clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Dendrogram After Cutting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "# Compute the flat clusters to confirm there are five clusters\n",
    "num_clusters = 5\n",
    "cluster_assignments = fcluster(h_cluster_model, t=num_clusters, criterion='maxclust')\n",
    "\n",
    "# Find the appropriate distance threshold for two clusters\n",
    "# The threshold is typically the height at which the tree splits into the desired number of clusters\n",
    "color_threshold = h_cluster_model[-(num_clusters - 1), 2]  \n",
    "\n",
    "# Plot the dendrogram with the color threshold\n",
    "plt.figure(figsize=(20, 7))\n",
    "dendrogram(\n",
    "    h_cluster_model,\n",
    "    labels=data['Country'].values,\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=5,\n",
    "    color_threshold=color_threshold\n",
    ")\n",
    "plt.title('Hierarchical Clustering Dendrogram (Colored by 5 Clusters)')\n",
    "plt.axhline(y=color_threshold, c='black', linestyle='--', lw=1)  # Add a line for the cut-off\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Clusterings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "h_clusters = cut_tree(h_cluster_model, n_clusters=k).flatten()\n",
    "\n",
    "# Calculate silhouette score\n",
    "silhouette_avg = silhouette_score(data_prep_scaled, h_clusters)\n",
    "\n",
    "# Calculate silhouette values for each sample\n",
    "silhouette_values = silhouette_samples(data_prep_scaled, h_clusters)\n",
    "\n",
    "# Print cluster sizes and average silhouette width for k=5\n",
    "unique, counts = np.unique(h_clusters, return_counts=True)\n",
    "cluster_sizes = dict(zip(unique, counts))\n",
    "\n",
    "avg_silhouette_widths = {}\n",
    "for cluster in unique:\n",
    "    # Get the silhouette values for the current cluster\n",
    "    cluster_silhouette_values = silhouette_values[h_clusters == cluster]\n",
    "    \n",
    "    # Calculate average silhouette width\n",
    "    avg_silhouette_widths[cluster] = cluster_silhouette_values.mean()\n",
    "\n",
    "print(f'Silhouette of {len(data)} units in {k} clusters:')\n",
    "print('Cluster sizes and average silhouette widths:')\n",
    "print(' '.join([str(cluster_sizes[cluster]) for cluster in unique]))\n",
    "print(' '.join([f'{avg_silhouette_widths[cluster]:.6f}' for cluster in unique]))\n",
    "print()\n",
    "\n",
    "# Calculate and print statistics for individual silhouette widths\n",
    "min_width = silhouette_values.min()\n",
    "q1_width = np.percentile(silhouette_values, 25)\n",
    "median_width = np.median(silhouette_values)\n",
    "mean_width = silhouette_values.mean()\n",
    "q3_width = np.percentile(silhouette_values, 75)\n",
    "max_width = silhouette_values.max()\n",
    "\n",
    "print('Individual silhouette widths:')\n",
    "print(f'    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.')\n",
    "print(f'{min_width:.6f} {q1_width:.6f} {median_width:.6f} {mean_width:.6f} {q3_width:.6f} {max_width:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, 21):\n",
    "    h_clusters = cut_tree(h_cluster_model, n_clusters=i).flatten()\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    silhouette_avg = silhouette_score(data_prep_scaled, h_clusters)\n",
    "    \n",
    "    print(f'k = {i}: Average Silhouette Score = {silhouette_avg:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
